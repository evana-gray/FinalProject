---
title: "Modeling"
format: html
editor: visual
---


```{r}
library(tidyverse)
library(tidymodels)
library(rsample)
library(parsnip)
library(tree)
library(rpart)
library(rpart.plot)
library(baguette)
library(ranger)
library(yardstick)
library(future)

set.seed(16)

plan(multisession, workers = 14)
```

#Read Data
```{r}
diabetes <- read_csv("diabetes_binary_health_indicators_BRFSS2015.csv")

#convert col names to lower case for easy reference
colnames(diabetes) <- tolower(colnames(diabetes))
```

#Create Factors
```{r}
diabetes <- diabetes |>
  mutate(
    diabetes_binary_f = factor(diabetes_binary, levels = c(0,1),
                               labels = c("no diabetes", "diabetes")), 
    highbp_f = factor(highbp, levels = c(0,1),
                               labels = c("no high BP", "high BP")),
    highchol_f = factor(highchol, levels = c(0,1),
                               labels = c("no high cholesterol", "high cholesterol")),
    cholcheck_f = factor(cholcheck, levels = c(0,1),
                               labels = c("no cholesterol check in 5 years", "yes cholesterol check in 5 years")),
    smoker_f = factor(smoker, levels = c(0,1),
                               labels = c("no", "yes")), 
    stroke_f = factor(stroke, levels = c(0,1),
                               labels = c("no", "yes")),
    heartdiseaseorattack_f = factor(heartdiseaseorattack, levels = c(0,1),
                               labels = c("no", "yes")),
    physactivity_f = factor(physactivity, levels = c(0,1),
                               labels = c("no", "yes")),
    fruits_f = factor(fruits, levels = c(0,1),
                               labels = c("no","yes")),
    veggies_f = factor(veggies, levels = c(0,1),
                               labels = c("no", "yes")),
    hvyalcoholconsump_f = factor(hvyalcoholconsump, levels = c(0,1),
                               labels = c("no", "yes")),
    anyhealthcare_f = factor(anyhealthcare, levels = c(0,1),
                               labels = c("no", "yes")),
    nodocbccost_f = factor(nodocbccost, levels = c(0,1),
                               labels = c("no","yes")),
    genhlth_f = factor(genhlth, levels = 1:5,
                               labels = c("excellent", "very good", "good", "fair", "poor")),
    diffwalk_f  = factor(diffwalk, levels = c(0,1),
                               labels = c("no", "yes")),
    sex_f = factor(sex, levels = c(0,1),
                               labels = c("female", "male")),
    age_f = factor(age, levels = 1:13,
                        labels = c("18 to 24",
                          "25 to 29", 
                          "30 to 34", 
                          "35 to 39",
                          "40 to 44",
                          "45 to 49",
                          "50 to 54",
                          "55 to 59",
                          "60 to 64",
                          "65 to 69",
                          "70 to 74",
                          "75 to 79", 
                          "80 to high"
                   )),
    education_f = factor(education, levels = 1:6,
                         labels = c("Never attended school or only kindergarten",
                         "Grades 1 through 8 (Elementary)",
                         "Grades 9 through 11 (Some high school)",
                         "Grade 12 or GED (High school graduate)",
                         "College 1 year to 3 years (Some college or technical school)",
                         "College 4 years or more (College graduate)")
                         ),
    income_f = factor(income, levels = 1:8,
                      labels = c("< 10k",
                                "< 15k",
                                "< 20k",
                                "< 25k",
                                "< 35k",
                                "< 50k",
                                "< 75k",
                                "75k+"))
  )
```

#Subset Variables of Interest

```{r}
diabetes_model_data <- diabetes |>
  select(diabetes_binary_f, highbp_f, sex_f, income_f, age_f, bmi)
rm(diabetes) #remove large original file
```


# Split Data And Fold
```{r}

diabetes_split <- rsample::initial_split(diabetes_model_data, prop = .7)
diabetes_train <- rsample::training(diabetes_split)
diabetes_test <- rsample::testing(diabetes_split)

diabetes_cv_folds <- vfold_cv(diabetes_train, 5)

```

# Recipe
```{r}
reci_1 <- recipe(diabetes_binary_f~ ., data = diabetes_train) |>
  step_normalize(all_numeric(), -diabetes_binary_f) |>
  update_role(diabetes_binary_f, new_role = "outcome") |>
  step_dummy(highbp_f, sex_f, income_f, age_f)


reci_1 |>
  prep(diabetes_train) |>
  bake(diabetes_train) |>
  colnames() 

```

# Classification Tree Models
 Changes to tuning parameter made to fit#_grid step
```{r}
# Define models
class_tree_mod <- decision_tree(tree_depth = tune(),
                                min_n = 20,
                                cost_complexity = tune()) |>
  set_engine("rpart") |>
  set_mode("classification")

#set up workflow
class_tree_flow <- workflow() |>
  add_recipe(reci_1) |>
  add_model(class_tree_mod)

#fit grids with multiple level parameters
fit1_grid <- grid_regular(cost_complexity(),
                           tree_depth(), 
                           levels = 2)
fit2_grid <- grid_regular(cost_complexity(),
                           tree_depth(), 
                           levels = 3)
fit3_grid <- grid_regular(cost_complexity(),
                           tree_depth(), 
                           levels = 5)
                          

#fit trees to CV fold
fit1 <- class_tree_flow |>
  tune_grid(resamples = diabetes_cv_folds,
            grid = fit1_grid,
            metrics = metric_set(mn_log_loss)
            )
fit2 <- class_tree_flow |>
  tune_grid(resamples = diabetes_cv_folds,
            grid = fit2_grid,
            metrics = metric_set(mn_log_loss)
            )
fit3 <- class_tree_flow |>
  tune_grid(resamples = diabetes_cv_folds,
            grid = fit3_grid,
            metrics = metric_set(mn_log_loss)
            )

#review model parameters
fits1_parameters <- fit1 |>
  collect_metrics() |>
  filter(.metric == "mn_log_loss") |>
  arrange(mean)

fits2_parameters <- fit2 |>
  collect_metrics() |>
  filter(.metric == "mn_log_loss") |>
  arrange(mean)

fits3_parameters <- fit3 |>
  collect_metrics() |>
  filter(.metric == "mn_log_loss") |>
  arrange(mean)

fits1_parameters
fits2_parameters
fits3_parameters

tree_results <- rbind(
  show_best(fit1, metric = "mn_log_loss", n = 1), # best metric for levels = 2
  show_best(fit2, metric = "mn_log_loss", n = 1), # best metric for levels = 3
  show_best(fit3, metric = "mn_log_loss", n = 1) # best metric for levels = 5 -- this is the model with the lowest mean log loss
)
tree_results

#select best fit
class_lowest_log <- select_best(fit3)
class_lowest_log

```

# Random Forest Models
```{r}

#define RF model
rf1_diabetes <- rand_forest(mtry = 2, trees = 100) |>
  set_engine("ranger") |>
  set_mode("classification")

rf2_diabetes <- rand_forest(mtry = 3, trees = 100) |>
  set_engine("ranger") |>
  set_mode("classification")

rf3_diabetes <- rand_forest(mtry = 5, trees = 100) |>
  set_engine("ranger") |>
  set_mode("classification")

rf4_diabetes <- rand_forest(mtry = tune(), trees = 100) |>
  set_engine("ranger") |>
  set_mode("classification")


#set up workflow
rf1_flow <- workflow() |>
  add_recipe(reci_1) |>
  add_model(rf1_diabetes)

rf2_flow <- workflow() |>
  add_recipe(reci_1) |>
  add_model(rf2_diabetes)

rf3_flow <- workflow() |>
  add_recipe(reci_1) |>
  add_model(rf3_diabetes)

rf4_flow <- workflow() |>
  add_recipe(reci_1) |>
  add_model(rf4_diabetes)


#fit RF to CV folds

rf1_fit <- rf1_flow |>
  tune_grid(resamples = diabetes_cv_folds,
            metrics = metric_set(mn_log_loss)
            )

rf2_fit <- rf2_flow |>
  tune_grid(resamples = diabetes_cv_folds,
            metrics = metric_set(mn_log_loss)
            )

rf3_fit <- rf3_flow |>
  tune_grid(resamples = diabetes_cv_folds,
            metrics = metric_set(mn_log_loss)
            )
rf4_fit <- rf4_flow |>
  tune_grid(resamples = diabetes_cv_folds,
            metrics = metric_set(mn_log_loss)
            )

# find best parameters
rf1_fits_parameters <- rf1_fit |>
  collect_metrics() |>
  filter(.metric == "mn_log_loss") |>
  arrange(mean)

rf2_fits_parameters <- rf2_fit |>
  collect_metrics() |>
  filter(.metric == "mn_log_loss") |>
  arrange(mean)

rf3_fits_parameters <- rf3_fit |>
  collect_metrics() |>
  filter(.metric == "mn_log_loss") |>
  arrange(mean)

rf4_fits_parameters <- rf4_fit |>
  collect_metrics() |>
  filter(.metric == "mn_log_loss") |>
  arrange(mean)

 rf1_fits_parameters # mtry= 2
 rf2_fits_parameters # mtry = 3
 rf3_fits_parameters # mtry = 5 -- this is the model with the lowest mean log loss
 rf4_fits_parameters # mtry = tune() -- best model mtry = 6 does not out perform mtry = 5

 rf3_lowest_log <- select_best(rf3_fit)
 rf3_lowest_log

```

#BEST Models on Full Split Dataset

```{r}
#Classification Tree - model 3 levels = 5
class_tree_flow2 <- class_tree_flow |>
  finalize_workflow(class_lowest_log)

class_final_fit <- class_tree_flow2 |>
  last_fit(diabetes_split, metrics = metric_set(accuracy, mn_log_loss))

collect_metrics(class_final_fit)


#Random Forest - model 3 mtry = 5
rf3_flow2 <- rf3_flow |> 
  finalize_workflow(rf3_lowest_log)

rf3_final_fit <- rf3_flow2 |>
  last_fit(diabetes_split, metrics = metric_set(accuracy, mn_log_loss))

collect_metrics(rf3_final_fit)


```


#Overall Best Model

The overall best model, with the lowest mn_log_loss is the random forest

## Confusion Matrix
```{r}

rf_lastfit <- extract_workflow(rf3_final_fit)

conf_mat(
  diabetes_model_data |>
    mutate(
      estimate = rf_lastfit |>
        predict(diabetes_model_data) |>
        pull()),
  diabetes_binary_f,
  estimate
    )


```


```{r}
#reset plan to default
plan(sequential)
```

